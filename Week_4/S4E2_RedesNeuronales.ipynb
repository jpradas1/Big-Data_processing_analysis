{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "#from pyspark.sql import SQLContext\n",
    "#sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bd5 = sqlContext.read.format(\n",
    "    \"com.databricks.spark.csv\"\n",
    ").option(\"header\", \"true\").load(\"bd5.csv\", inferSchema=True)\n",
    "sqlContext.registerDataFrameAsTable(bd5, \"bd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 'int'),\n",
       " ('MONTH', 'int'),\n",
       " ('DAY_OF_MONTH', 'int'),\n",
       " ('DAY_OF_WEEK', 'int'),\n",
       " ('CRS_DEP_TIME', 'int'),\n",
       " ('OP_UNIQUE_CARRIER', 'string'),\n",
       " ('TAIL_NUM', 'string'),\n",
       " ('ARR_DELAY', 'double'),\n",
       " ('DEP_DELAY', 'double'),\n",
       " ('ORIGIN', 'string'),\n",
       " ('DEST', 'string'),\n",
       " ('DISTANCE', 'double'),\n",
       " ('CANCELLED', 'double'),\n",
       " ('DIVERTED', 'double'),\n",
       " ('CARRIER_DELAY', 'double'),\n",
       " ('WEATHER_DELAY', 'double'),\n",
       " ('NAS_DELAY', 'double'),\n",
       " ('SECURITY_DELAY', 'double'),\n",
       " ('LATE_AIRCRAFT_DELAY', 'double'),\n",
       " ('LogD', 'double'),\n",
       " ('Retraso', 'int'),\n",
       " ('RetrasoNeto', 'double'),\n",
       " ('Horario', 'int')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd5.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol='OP_UNIQUE_CARRIER',outputCol='IndexUniqueCarrier') #el índice empieza en el 0!\n",
    "bd6=indexer.fit(bd5).transform(bd5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "a1  = VectorAssembler(\n",
    "    inputCols=['DEP_DELAY','DISTANCE','DAY_OF_WEEK',\n",
    "               'CRS_DEP_TIME','IndexUniqueCarrier'],\n",
    "    outputCol='features')\n",
    "\n",
    "bd7 = a1.transform(bd6).select(col(\"Retraso\").cast('double').alias(\"label\"),'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partición Test - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21278\n",
      "9188\n"
     ]
    }
   ],
   "source": [
    "(bd_train, bd_test) = bd7.randomSplit([0.7, 0.3],seed=123)\n",
    "print(bd_train.count())\n",
    "print(bd_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"label\",\n",
    "      featuresCol=\"features\", \n",
    "      maxIter=100, \n",
    "      layers=[5, 5, 2], \n",
    "      seed=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El numéro de Neuronas de la 1a capa = al número de elementos feature\n",
    "* El numéro de Neuronas de la última capa = al número de labels\n",
    "* Las neuronas internas tienen función de activación sigmoide \n",
    "* Las neuronas de la última capa tienen función de activación softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mlp.fit(bd_train)\n",
    "\n",
    "pred = model.transform(bd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|            features|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|[-20.0,602.0,3.0,...|       0.0|\n",
      "|  0.0|[-16.0,641.0,1.0,...|       0.0|\n",
      "|  0.0|[-16.0,641.0,6.0,...|       0.0|\n",
      "|  0.0|[-16.0,868.0,6.0,...|       0.0|\n",
      "|  0.0|[-16.0,888.0,3.0,...|       0.0|\n",
      "|  0.0|[-15.0,731.0,1.0,...|       0.0|\n",
      "|  0.0|[-15.0,888.0,4.0,...|       0.0|\n",
      "|  0.0|[-15.0,888.0,5.0,...|       0.0|\n",
      "|  0.0|[-15.0,967.0,3.0,...|       0.0|\n",
      "|  0.0|[-15.0,1464.0,6.0...|       0.0|\n",
      "|  0.0|[-14.0,236.0,3.0,...|       0.0|\n",
      "|  0.0|[-14.0,236.0,4.0,...|       0.0|\n",
      "|  0.0|[-14.0,337.0,1.0,...|       0.0|\n",
      "|  0.0|[-14.0,337.0,1.0,...|       0.0|\n",
      "|  0.0|[-14.0,337.0,1.0,...|       0.0|\n",
      "|  0.0|[-14.0,337.0,2.0,...|       0.0|\n",
      "|  0.0|[-14.0,414.0,2.0,...|       0.0|\n",
      "|  0.0|[-14.0,628.0,1.0,...|       0.0|\n",
      "|  0.0|[-14.0,731.0,5.0,...|       0.0|\n",
      "|  0.0|[-14.0,868.0,3.0,...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0| 4383|\n",
      "|  0.0|       0.0|14806|\n",
      "|  0.0|       1.0|  580|\n",
      "|  1.0|       0.0| 1509|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.groupBy('label','prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901823479650343"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator as MCCE\n",
    "\n",
    "evaluator = MCCE(metricName=\"precision\")\n",
    "evaluator.evaluate(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9006312581628211"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = model.transform(bd_test)\n",
    "evaluator.evaluate(pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuneado Automático de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código válido para Pyspark v2.0.0 o superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit\n",
    "    \n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "#lgr = LogisticRegression(maxIter=10, \n",
    "     #labelCol=\"label\", \n",
    "    # featuresCol=\"features\")\n",
    "                        \n",
    "#paramGrid = ParamGridBuilder()\\\n",
    "    #.addGrid(lgr.regParam, [1,0.1]) \\\n",
    "    #.addGrid(lgr.elasticNetParam, [0.0, 1.0])\\\n",
    "    # .build()\n",
    "    \n",
    "#tvs = TrainValidationSplit(estimator=lgr,\n",
    "                          # estimatorParamMaps=paramGrid,\n",
    "                          # evaluator=BCE(metricName=\"areaUnderROC\"),\n",
    "                           #trainRatio=0.8)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = tvs.fit(bd_train)\n",
    "\n",
    "#pred = model.transform(bd_test)\n",
    "#pred.select(\"features\", \"label\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BCE(metricName=\"areaUnderROC\").evaluate(pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
